\documentclass{report}

\usepackage[warn]{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{luainputenc}
\usepackage[english, russian]{babel}
\usepackage[pdftex]{hyperref}
\usepackage{tempora}
\usepackage[12pt]{extsizes}
\usepackage{listings}
\usepackage{color}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath}

\geometry{a4paper,top=2cm,bottom=2cm,left=2.5cm,right=1.5cm}
\setlength{\parskip}{0.5cm}
\setlist{nolistsep, itemsep=0.3cm,parsep=0pt}

\usepackage{listings}
\lstset{language=C++,
        basicstyle=\footnotesize,
		keywordstyle=\color{blue}\ttfamily,
		stringstyle=\color{red}\ttfamily,
		commentstyle=\color{green}\ttfamily,
		morecomment=[l][\color{red}]{\#}, 
		tabsize=4,
		breaklines=true,
  		breakatwhitespace=true,
  		title=\lstname,       
}

\makeatletter
\renewcommand\@biblabel[1]{#1.\hfil}
\makeatother

\begin{document}

\begin{titlepage}

\begin{center}
Министерство науки и высшего образования Российской Федерации
\end{center}

\begin{center}
Федеральное государственное автономное образовательное учреждение высшего образования \\
Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского
\end{center}

\begin{center}
Институт информационных технологий, математики и механики
\end{center}

\vspace{4em}

\begin{center}
\textbf{\LargeОтчет по лабораторной работе} \\
\end{center}
\begin{center}
\textbf{\Large«Поразрядная сортировка для целых чисел с четно-нечетным слиянием Бэтчера.»} \\
\end{center}

\vspace{4em}

\newbox{\lbox}
\savebox{\lbox}{\hbox{text}}
\newlength{\maxl}
\setlength{\maxl}{\wd\lbox}
\hfill\parbox{7cm}{
\hspace*{5cm}\hspace*{-5cm}\textbf{Выполнила:} \\ студент группы 381906-2 \\ Зотов М.С.\\
\\
\hspace*{5cm}\hspace*{-5cm}\textbf{Проверил:}\\ доцент кафедры МОСТ, \\ кандидат технических наук \\ Сысоев А. В.\\
}
\vspace{\fill}

\begin{center} Нижний Новгород \\ 2022 \end{center}

\end{titlepage}

\setcounter{page}{2}

% Содержание
\tableofcontents
\newpage

% Введение
\section*{Введение}
\addcontentsline{toc}{section}{Введение}
\par Поразрядная сортировка - это несравнительный алгоритм, т.е. алгоритм не сортирует элементы, сравнивая их между собой, а перебирает по порядку и  группирует по определённому признаку, в моём случае этим признаком является разряд числа. Поразрядная сортировка применима к лексиграфическим типам данных, такие как числа и строки. В данной лабораторной работе типом данных является целое число, которое легко можно поделить на разряды: единицы, десятки,сотни, тысячи и т.д. Поскольку сравнивать разряды между собой можно начинать как с большего, так и с меньшего, то поразрядная сортировка имеет две реализации, а именно LSD(least significant digit) и MSD(most significant digit). LSD сортировка начинается с наименее значащего разряда, а MSD с наиболее значащего.  Относительно целых чисел гораздо удобнее будет использовать LSD реализацию.

\newpage

% Постановка задачи
\section*{Постановка задачи}
\addcontentsline{toc}{section}{Постановка задачи}
\par Цель лабораторной рботы состоит в следующих задачах:
\begin{itemize}
\item Реализация последовательного алгоритма поразрядной сортировки.
\item Реализация параллельного алгоритма поразрядной сортировки используя OpenMP.
\item Реализация параллельного алгоритма поразрядной сортировки используя TBB.
\item Исследование времени выполнения алгоритмов и сравнение их эффективности между собой.
\item Сделать выводы на основе выполненных тестов и экспериментов.
\end{itemize}
Так же необходимо убедиться в корректности работы каждой программы. Для этого необходимо для каждой версии реализации написать ряд тестов с использованием Google C++ Testing Framework.
\newpage

% Описание алгоритма
\section*{Описание алгоритма}
\addcontentsline{toc}{section}{Описание алгоритма}
\par Поразрядная сортировка заключается в том, что элементы перебираются по порядку и группируются по значению цифры в самом младшем разряде, затем выстраивается новая последовательность данных, а именно, начиная с нулевой группы, мы достаём числа и заносим в новый массив, затем достаём числа из первой группы, потом из второй, из третей, и так до девятой группы. После, уже для новго массива данных, нам нужно снова перебрать последовательность и сгруппировать по значению цифры второго разряда,затем по третьему разряду и так до самого наибольшего разряда. В итоге будем иметь отсортированный массив данных, полученный поразрядной сортировкой со сложностью O($n*k$), где $k$ - максимальный разряд числа.
\newpage

% Описание схемы распараллеливания
\section*{Описание схемы распараллеливания}
\addcontentsline{toc}{section}{Описание схемы распараллеливания}
\par Суть распараллеливания поразрядной сортировки заключается в том, что изначальный массив данных разбивается на части и каждый поток работает, только с теми данными, которые ему достались после разбиения основного массива. Затем каждый поток применяет поразрядную сортировку к своей части данных и отсортированные части сливаются в один массив при помощи слияния Бэтчера. В итоге основной поток будет иметь отсортированный массив данных.
\newpage

% Описание программной реализации
\section*{Описание программной реализации}
\addcontentsline{toc}{section}{Описание программной реализации}
\subsection*{OpenMP}
\addcontentsline{toc}{subsection}{OpenMP}
\par Программа, реализующая поразрядную сортировку с помощью технологии omp, состоит из одного заголовочного файла sort\_with\_batcher\_merge.h и двух файлов исходного кода: sort\_with\_batcher\_merge.cpp и main.cpp.
\par В заголовочном файле определены прототипы следующих функций:
\par Создание и заполнение вектора случайными данными \par
\begin{lstlisting}
std::vector<int> getRandomVector(int size);
\end{lstlisting}
\par Последовательная реализация поразрядной сортировки \par
\begin{lstlisting}
void radixSort(std::vector<int>* main_data, int size, int offset);
\end{lstlisting}
\par Функция поиска максимального разряда в массиве \par
\begin{lstlisting}
int getMaxDigit(const int data);
\end{lstlisting}
\par Параллельная реализация поразрядной сортировки \par
\begin{lstlisting}
void parallelRadixSort(std::vector<int>* data, int size, int ThreadNum);
\end{lstlisting}
\par В файле  sort\_with\_batcher\_merge.cpp определены реализации этих функций. Для распараллеливания алгоритма при помощи технологии OMP использовалась директива препроцессора \verb|#pragma omp parallel private(ThreadRank)| и \verb|#pragma omp parallel for|. Перед выполнением первой директивы создаётся и заполняется 2 вектора,размер которых равен числу потоков, которые будут хранить количество данных для каждого потока и смещение относительно исходного вектора данных. Код первой директивы будет выполнен каждым потоком, с учётом того, что переменная ThreadRank- частная, т.е. у каждого потока своя. Каждый поток применяет последовательную поразрядную сортировку к той части данных, которая ему досталась. Слияние Бэтчера реализовано с помощью цикла: сначала выполняется параллельная зона, в которой каждый чётный поток и его сосед справа упорядочено сливают свои данные в один вектор, и специальная функция обходит этот вектор, что бы убедиться, что вектор отсортирован. Затем выход из параллельной зоны и перераспределение данных двух векторов sendCount и displ, которые нужны для навигации в основном векторе данных. Затем пересчитывается число потоков, как правило вдвое меньшее, и выполняется следующая итерация цикла. Слияние Бэтчера реализовано с помощью следующих функций:
\par Слияние Бэтчера для нечётных элементов массива \par
\begin{lstlisting}
void oddMerge(std::vector<int>* data, int f\_size, int f_offset, int s_size, int s_offset)
\end{lstlisting}
\par Слияние Бэтчера для чётных элементов массива \par
\begin{lstlisting}
void evenMerge(std::vector<int>* data, int f\_size, int f_offset, int s_size, int s_offset)
\end{lstlisting}
\par Разделение элементов массива во временную память \par
\begin{lstlisting}
void splitData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ)
\end{lstlisting}
\par Перераспределение данных в основной вектор из временной памяти в упорядоченном виде \par
\begin{lstlisting}
void mergeData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ)
\end{lstlisting}
\par Функция менняет данные местами, если двруг они окажутся не упорядоченными \par
\begin{lstlisting}
void oddMerge(std::vector<int>* data, int f_size, int f_offset, int s_size, int s_offset)
\end{lstlisting}

\subsection*{TBB}
\addcontentsline{toc}{subsection}{TBB}
\par Для реализации параллельного алгоритма с помошью библиотеки TBB необходимо инициализировать экземпляр класса \verb|tbb::task_scheduler_init|, который служит для создания потоков и внутренних структур, необходимых планировщику потоков для работы. После этого, удобно использовать шаблонную функцию \verb|tbb::parallel_for| для реализациия распараллеливания циклов с известным числом повторений.Первый аргумент функции это некий объект - итерационное пространство, а второй это класс-функтор.
\par В моём случае интерационное пространство выглядит следующим образом:
\begin{lstlisting}
tbb::blocked_range<int>(0, number_threads, 1)
\end{lstlisting}
\par Это представляет собой диапазон вида полуинтервала [begin,end), где begin- 0, а end- число потоков.
\par Второй параметр - функтор, класс реализующий вычисления цикла через метод \verb|body::operator()|.
\par Я реализовал класс Data выступающий в роли функтора, он содержит 3 поля, конструктор и оператор круглых скобок:
\begin{lstlisting}
class Data {
 private:
    std::vector<int>* mainData;
    std::vector<int>* localData;
    std::vector<int>* localSize;
 public:
    Data(std::vector<int>* mainData_, std::vector<int>* localData_, std::vector<int>* localSize_):
        mainData(mainData_), localData(localData_), localSize(localSize_) {}
    void operator()(const tbb::blocked_range<int>& range)const {
        for (int i = range.begin(); i != range.end(); i++) {
            radixSort(mainData, (*localData)[i], (*localSize)[i]);
        }
    }
};
\end{lstlisting}
\par Первая переменная содержит ссылку на изначальный вектор данных, вторая содержит размер доступных данных для каждого потока, третья указывает смещение в основном векторе для каждого потока. Внутри оператора круглые скобки выполняется последовательный алгоритм поразрядной сортировки над своими доступными данными каждым потоком.
\par После выхода из параллельной зоны мы имеем некоторое количество отсортированных векторов данных, которые последовательно сливаются в один вектор с помощью слияния Бэтчера.

\newpage

% Подтверждение корректности
\section*{Подтверждение корректности}
\addcontentsline{toc}{section}{Подтверждение корректности}
Для подтверждения корректности в программе реализован набор тестов, разработанных при помощи библиотеки для модульного тестирования Google C++ Testing Framework. Проверяются случаи сортировки векторов с использованием разного количества потоков и разного размера исходного вектора данных.
\par Успешное прохождение всех тестов доказывает корректность работы работы программы.

\newpage

% Результаты экспериментов
\section*{Результаты экспериментов}
\addcontentsline{toc}{section}{Результаты экспериментов}
Вычислительные эксперименты для оценки эффективности параллельного варианта
сортировки Шелла простым слиянием проводились на ПК со следующими характеристиками:
\begin{itemize}
\item Процессор: AMD Ryzen 7 7200X Eight-Core Processor ;
\item Оперативная память: 16 ГБ (DDR4), 2.133GHz;
\item Число ядер: 8
\item Операционная система: Windows 10 Домашняя.
\end{itemize}

\par Эксперименты проводились на 2,4,8 потоках с векторами размера 1000000 элементов (см. Таблицу 1), и 10000000 элементов (см. Таблицу 2).

\par Результаты экспериментов представлены в Таблице 1 и в Таблице 2.

\begin{table}[!h]
\caption{Результаты вычислений для вектора длиной 1000000 элементов}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{3}{*}
	{\begin{tabular}[c]{@{}c@{}}Число\\ потоков\end{tabular}} & 
\multirow{2}{*}
	{\begin{tabular}[c]{@{}c@{}}Последовательный\\ алгоритм\end{tabular}} & 
\multicolumn{4}{c|}
	{Параллельный алгоритм}	\\ 
	\cline{3-6} & & 
	\multicolumn{2}{c|}{OpenMP} & 
	\multicolumn{2}{c|}{TBB} 
	\\ \cline{2-6}
	& t, с	    & t, с & speedup		& t, с & boost			\\ \hline
2   & 0.0681634     & 0.0516512 & 1.31969        	& 0.049631 & 1.4274            \\ \hline
4   & 0.0696107     & 0.0338858 & 2.05427       	& 0.0455542 & 1.74374                  \\ \hline
8    & 0.0803702 & 0.0355569 &  2.26033      	& 0.052243 &  1.47213  	         \\ \hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Результаты вычислений для вектора длиной 10000000 элементов}
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow{3}{*}
	{\begin{tabular}[c]{@{}c@{}}Число\\ потоков\end{tabular}} & 
\multirow{2}{*}
	{\begin{tabular}[c]{@{}c@{}}Последовательный\\ алгоритм\end{tabular}} & 
\multicolumn{4}{c|}
	{Параллельный алгоритм}	\\ 
	\cline{3-6} & & 
	\multicolumn{2}{c|}{OpenMP} & 
	\multicolumn{2}{c|}{TBB} 
	\\ \cline{2-6}
	& t, с	    & t, с & speedup		& t, с & boost			\\ \hline
2   & 0.818862     & 0.481481 & 1.70072        	& 0.523226 & 1.57497            \\ \hline
4   & 0.837596     & 0.33399 & 2.50785       	& 0.422548 & 1.91823                  \\ \hline
8    & 0.822892 & 0.315079 &  2.6117      	& 0.434528 &  1.89554  	         \\ \hline
\end{tabular}
\end{table}

\newpage

% Выводы из результатов экспериментов
\section*{Выводы из результатов экспериментов}
\addcontentsline{toc}{section}{Выводы из результатов экспериментов}
Исходя из данных полученных в результате экспериментов, можно сделать вывод, что параллельная реализация алгоритма поразрядной сортировки с слиянием Бэчера работает быстрее последовательной. Чем больше размер вектора данных, тем значительнее прирост в производительности и тем выше ускорение. Также, можно заметить, что OpenMP релизация работает быстрее, чем TBB версия. Это связано с тем что в первой реализации слияние Бэтчера выполняется параллельно, а во второй последовательно. Количество потоков выполняющих сортировку, также влияет на эффективность алгоритма.
\newpage

% Заключение
\section*{Заключение}
\addcontentsline{toc}{section}{Заключение}
В результате выполнения лабораторной работы были  разработаны последовательная и параллельные реализации алгоритма поразрядной сортировки с слиянием Бэтчера. На основании полученных данных из тестов убедились в корректности разработанных алгоритмов. Параллельные версии алгоритма ожидаемо оказались быстрее чем последовательная, что доказывает успешное выполнение поставленных задач лабораторной работы.
\newpage

% Литература
\section*{Литература}
\addcontentsline{toc}{section}{Литература}
\begin{enumerate}
\item Образовательный комплекс «Введение в принципы функционирования и применения современных мультиядерных архитектур (на примере Intel Xeon Phi)» / сост.: Сиднев А.А., Сысоев А.В., Мееров И.Б. , 2013
\item Учебный курс «Средства разработки параллельных программ для систем с общей памятью. Библиотека Intel Threading Building Blocks»  / сост.: А.А. Сиднев, А.В. Сысоев, И.Б. Мееров - Нижний Новгород, 2007 
\item Учебный курс "Введение в методы параллельного программирования" Раздел "Параллельное программирование с использованием OpenMP"  / сост.: В.П. Гергель - Нижний Новгород, 2007 
\item Алгоритм поразрядной сортировки \newline URL: https://intuit.ru/studies/courses/10612/1096/lecture/22924?page=3
\end{enumerate} 
\newpage

% Приложение
\section*{Приложение}
\addcontentsline{toc}{section}{Приложение}
Исходный код программ.
\par Реализация с использованием технологии OpenMP:
\begin{lstlisting}
// sort_with_batcher_merge.h

// Copyright 2022 Zotov Maxim
#ifndef MODULES_TASK_2_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_
#define MODULES_TASK_2_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_

#include <vector>
#include <string>

int getMaxDigit(const int data);
std::vector<int> getRandomVector(int size);
void radixSort(std::vector<int>* main_data, int size, int offset);
void parallelRadixSort(std::vector<int>* data, int size, int ThreadNum);
#endif  // MODULES_TASK_2_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_

\end{lstlisting}
\begin{lstlisting}
// sort_with_batcher_merge.cpp

// Copyright 2022 Zotov Maxim
#include <omp.h>
#include <vector>
#include <string>
#include <random>
#include <iostream>
#include "../../../modules/task_2/zotov_m_sort_with_batcher_merge/sort_with_batcher_merge.h"

const int MAX = 5000;

std::vector<int> getRandomVector(int size) {
    std::mt19937 generate;
    generate.seed(time(0));
    std::uniform_real_distribution<> uid(-MAX, MAX);
    std::vector<int> tmp;
    for (int i = 0; i < size; i++) {
        tmp.push_back(uid(generate));
    }
    return tmp;
}

int getMaxDigit(const int max) {
    int tmp = 0;
    int i = 0;
    for (i = 1; i < 10; i++) {
        tmp = max / static_cast<int>(pow(10, i));
        if (tmp == 0) {
            break;
        }
    }
    return i;
}

void  radixSort(std::vector<int>* main_data, int size, int offset) {
    std::vector<int> sorted_data[19];
    int koef = 0;
    int max_digit = getMaxDigit(MAX);
    while (koef < max_digit) {
        for (int i = offset; i < offset + size; i++) {
            int digit = (*main_data)[i] / pow(10, koef);
            digit = digit % 10;
            sorted_data[digit + 9].push_back((*main_data)[i]);
        }

        int iter = 0;
        for (int i = 0; i < 19; i++) {
            for (int j = 0; j < static_cast<int>(sorted_data[i].size()); j++) {
                (*main_data)[offset + iter] = sorted_data[i][j];
                iter++;
            }
            sorted_data[i].clear();
        }
        koef++;
    }
}

int getNumberOfIterations(int tN) {
    int k = 1;
    while (tN > std::pow(2, k)) {
        k++;
    }
    return k;
}

void splitData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ) {
    while (f < f_displ && s < s_displ) {
        if ((*data)[f] < (*data)[s]) {
            first->push_back((*data)[f]);
            f += 2;
        } else {
            first->push_back((*data)[s]);
            s += 2;
        }
    }

    if (f >= f_displ) {
        for (int j = s; j < s_displ; j += 2) {
            first->push_back((*data)[j]);
        }
    } else if (s >= s_displ) {
        for (int j = f; j < f_displ; j += 2) {
            first->push_back((*data)[j]);
        }
    }
}

void mergeData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ) {
    int i = 0;

    while (f < f_displ) {
        (*data)[f] = (*first)[i];
        f += 2;
        i++;
    }

    while (s < s_displ) {
        (*data)[s] = (*first)[i];
        s += 2;
        i++;
    }
}


void oddMerge(std::vector<int>* data, int f_size, int f_offset, int s_size, int s_offset) {
    int f = f_offset + 1, s = s_offset + 1;
    int f_displ = f_size + f_offset, s_displ = s_size + s_offset;
    std::vector<int> first;

    splitData(data, &first, f, f_displ, s, s_displ);
    mergeData(data, &first, f, f_displ, s, s_displ);
}

void evenMerge(std::vector<int>* data, int f_size, int f_offset, int s_size, int s_offset ) {
    int f = f_offset, s = s_offset;
    int f_displ = f_size + f_offset, s_displ = s_size + s_offset;
    std::vector<int> first;

    splitData(data, &first, f, f_displ, s, s_displ);
    mergeData(data, &first, f, f_displ, s, s_displ);
}

void compare(std::vector<int>* data, int size, int offset) {
    for (int i = offset; i < size + offset; i++) {
        if ((*data)[i] > (*data)[i + 1]) std::swap((*data)[i], (*data)[i + 1]);
    }
}

void parallelRadixSort(std::vector<int>* data, int size, int ThreadNum) {
    int local_size = size / ThreadNum;
    int remain = size % ThreadNum;
    std::vector<int> sendCount(ThreadNum);
    std::vector<int> displ;
    int iterator;
    int ThreadRank;
    int sum = 0;
    for (int i = 0; i < ThreadNum; i++) {
        sendCount[i] = local_size;
        if (remain > 0) {
            sendCount[i]++;
            remain--;
        }
        displ.push_back(sum);
        sum += sendCount[i];
    }

    iterator = getNumberOfIterations(ThreadNum);
    omp_set_num_threads(ThreadNum);

    #pragma omp parallel private(ThreadRank)
    {
    ThreadRank = omp_get_thread_num();
    radixSort(data, sendCount[ThreadRank], displ[ThreadRank]);
    #pragma omp barrier
    }

    for (int i = 0; i < iterator; i++) {
        omp_set_num_threads(ThreadNum);
        #pragma omp parallel private(ThreadRank)
        {
            ThreadRank = omp_get_thread_num();

            if (ThreadRank % 2 == 0 && ThreadRank + 1 < ThreadNum) {
                evenMerge(data, sendCount[ThreadRank], displ[ThreadRank],
                    sendCount[ThreadRank + 1], displ[ThreadRank + 1]);
            } else if (ThreadRank % 2 == 1) {
                oddMerge(data, sendCount[ThreadRank - 1], displ[ThreadRank - 1],
                    sendCount[ThreadRank], displ[ThreadRank]);
            }
            #pragma omp barrier
            if (ThreadRank % 2 == 0 && ThreadRank + 1 < ThreadNum) {
                compare(data, sendCount[ThreadRank], displ[ThreadRank]);
            } else if (ThreadRank % 2 == 1) {
                compare(data, sendCount[ThreadRank] - 1 , displ[ThreadRank]);
            }

            #pragma omp barrier
            #pragma omp master
            {
                for (int j = 0; j < ThreadNum - 1 ; j += 2) {
                    sendCount[j/2] = sendCount[j] + sendCount[j + 1];
                    displ[j/2] = displ[j];
                }
                if (ThreadNum % 2 == 1) {
                    sendCount[(ThreadNum - 1) / 2] = sendCount[ThreadNum - 1];
                    displ[(ThreadNum - 1) / 2] = displ[ThreadNum - 1];
                    ThreadNum = ThreadNum / 2 + 1;
                } else {
                    ThreadNum /= 2;
                }
            }
        }
    }
}

\end{lstlisting}
\begin{lstlisting}
// main.cpp

// Copyright 2022 Zotov Maxim
#include <gtest/gtest.h>
#include <vector>
#include "./sort_with_batcher_merge.h"


TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_1) {
    std::vector<int> vec = { 6, 5, -4, 3, -2, 1, -8 };
    std::vector<int> sort_vec = { -8, -4, -2, 1, 3, 5, 6 };
    int number_threads = 2;
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_2) {
    int size = 20;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> std_sort_vec = vec;
    int number_threads = 2;
    std::sort(std_sort_vec.begin(), std_sort_vec.end());
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(std_sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_3) {
    int size = 20;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> std_sort_vec = vec;
    int number_threads = 5;
    std::sort(std_sort_vec.begin(), std_sort_vec.end());
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(std_sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_4) {
    int size = 100;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 3;
    radixSort(&seq_vec, seq_vec.size(), 0);
    parallelRadixSort(&vec, vec.size(),  number_threads);
    ASSERT_EQ(seq_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_5) {
    int size = 100;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 10;
    radixSort(&seq_vec, seq_vec.size(), 0);
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(seq_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_6) {
    int size = 10000;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 13;
    radixSort(&seq_vec, seq_vec.size(), 0);
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(seq_vec, vec);
}

int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}
\end{lstlisting}
\begin{lstlisting}
// sort_with_batcher_merge.h

// Copyright 2022 Zotov Maxim
#ifndef MODULES_TASK_3_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_
#define MODULES_TASK_3_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_

#include <vector>
#include <string>

int getMaxDigit(std::vector<int> data);
std::vector<int> getRandomVector(int size);
void radixSort(std::vector<int>* main_data, int size, int offset);
void parallelRadixSort(std::vector<int>* data, int size, int ThreadNum);

#endif  // MODULES_TASK_3_ZOTOV_M_SORT_WITH_BATCHER_MERGE_SORT_WITH_BATCHER_MERGE_H_
\end{lstlisting}
\begin{lstlisting}
// sort_with_batcher_merge.cpp

// Copyright 2022 Zotov Maxim
#include <tbb/tbb.h>
#include <vector>
#include <random>
#include <iostream>
#include "../../../modules/task_3/zotov_m_sort_with_batcher_merge/sort_with_batcher_merge.h"

const int MAX = 30000;

std::vector<int> getRandomVector(int size) {
    std::mt19937 generate;
    generate.seed(time(0));
    std::uniform_real_distribution<> uid(-MAX, MAX);
    std::vector<int> tmp;
    for (int i = 0; i < size; i++) {
        tmp.push_back(uid(generate));
    }
    return tmp;
}

int getMaxDigit(std::vector<int> data) {
    int max_digit = 0;
    int size = data.size();
    while (size > 0) {
        max_digit++;
        for (size_t i = 0; i < data.size(); i++) {
            int div = data[i] / powf(10, max_digit);
            if (div == 0)
                size--;
        }
    }

    return max_digit;
}

void  radixSort(std::vector<int>* main_data, int size, int offset) {
    std::vector<int> sorted_data[19];
    int koef = 0;
    int max_digit = getMaxDigit(*main_data);
    while (koef < max_digit) {
        for (int i = offset; i < offset + size; i++) {
            int digit = (*main_data)[i] / pow(10, koef);
            digit = digit % 10;
            sorted_data[digit + 9].push_back((*main_data)[i]);
        }

        int iter = 0;
        for (int i = 0; i < 19; i++) {
            for (int j = 0; j < static_cast<int>(sorted_data[i].size()); j++) {
                (*main_data)[offset + iter] = sorted_data[i][j];
                iter++;
            }
            sorted_data[i].clear();
        }
        koef++;
    }
}

int getNumberOfIterations(int tN) {
    int k = 1;
    while (tN > std::pow(2, k)) {
        k++;
    }
    return k;
}

void splitData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ) {
    while (f < f_displ && s < s_displ) {
        if ((*data)[f] < (*data)[s]) {
            first->push_back((*data)[f]);
            f += 2;
        } else {
            first->push_back((*data)[s]);
            s += 2;
        }
    }

    if (f >= f_displ) {
        for (int j = s; j < s_displ; j += 2) {
            first->push_back((*data)[j]);
        }
    } else if (s >= s_displ) {
        for (int j = f; j < f_displ; j += 2) {
            first->push_back((*data)[j]);
        }
    }
}

void mergeData(std::vector<int>* data, std::vector<int>* first, int f, int f_displ, int s, int s_displ) {
    int i = 0;

    while (f < f_displ) {
        (*data)[f] = (*first)[i];
        f += 2;
        i++;
    }

    while (s < s_displ) {
        (*data)[s] = (*first)[i];
        s += 2;
        i++;
    }
}


void oddMerge(std::vector<int>* data, int f_size, int f_offset, int s_size, int s_offset) {
    int f = f_offset + 1, s = s_offset + 1;
    int f_displ = f_size + f_offset, s_displ = s_size + s_offset;
    std::vector<int> first;

    splitData(data, &first, f, f_displ, s, s_displ);
    mergeData(data, &first, f, f_displ, s, s_displ);
}

void evenMerge(std::vector<int>* data, int f_size, int f_offset, int s_size, int s_offset) {
    int f = f_offset, s = s_offset;
    int f_displ = f_size + f_offset, s_displ = s_size + s_offset;
    std::vector<int> first;

    splitData(data, &first, f, f_displ, s, s_displ);
    mergeData(data, &first, f, f_displ, s, s_displ);
}

void compare(std::vector<int>* data, int size, int offset) {
    for (int i = offset; i < size + offset; i++) {
        if ((*data)[i] > (*data)[i + 1]) std::swap((*data)[i], (*data)[i + 1]);
    }
}

class Data {
 private:
    std::vector<int>* mainData;
    std::vector<int>* localData;
    std::vector<int>* localSize;
 public:
    Data(std::vector<int>* mainData_, std::vector<int>* localData_, std::vector<int>* localSize_):
        mainData(mainData_), localData(localData_), localSize(localSize_) {}
    void operator()(const tbb::blocked_range<int>& range)const {
        for (int i = range.begin(); i != range.end(); i++) {
            radixSort(mainData, (*localData)[i], (*localSize)[i]);
        }
    }
};

void parallelRadixSort(std::vector<int>* data, int size, int number_threads) {
    int local_size = size / number_threads;
    int remain = size % number_threads;
    std::vector<int> sendCount(number_threads);
    std::vector<int> displ;
    int iterator;
    int sum = 0;
    for (int i = 0; i < number_threads; i++) {
        sendCount[i] = local_size;
        if (remain > 0) {
            sendCount[i]++;
            remain--;
        }
        displ.push_back(sum);
        sum += sendCount[i];
    }
    iterator = getNumberOfIterations(number_threads);
    tbb::task_scheduler_init init(number_threads);
    tbb::parallel_for(tbb::blocked_range<int>(0, number_threads, 1),
        Data(data, &sendCount, &displ),
        tbb::auto_partitioner());
    init.terminate();
    for (int j = 0; j < iterator; j++) {
        for (int i = 0; i < number_threads; i += 2) {
            if (i + 1 < number_threads) {
                evenMerge(data, sendCount[i], displ[i], sendCount[i + 1], displ[i + 1]);
                oddMerge(data, sendCount[i], displ[i], sendCount[i + 1], displ[i + 1]);
                compare(data, sendCount[i], displ[i]);
                compare(data, sendCount[i + 1] - 1, displ[i + 1]);
            }
        }

        for (int j = 0; j < number_threads - 1; j += 2) {
            sendCount[j / 2] = sendCount[j] + sendCount[j + 1];
            displ[j / 2] = displ[j];
        }
        if (number_threads % 2 == 1) {
            sendCount[(number_threads - 1) / 2] = sendCount[number_threads - 1];
            displ[(number_threads - 1) / 2] = displ[number_threads - 1];
            number_threads = number_threads / 2 + 1;
        } else {
            number_threads /= 2;
        }
    }
}

\end{lstlisting}
\begin{lstlisting}
// main.cpp

// Copyright 2022 Zotov Maxim
#include <gtest/gtest.h>
#include <omp.h>
#include <vector>
#include "./sort_with_batcher_merge.h"

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_1) {
    std::vector<int> vec = { 6, 5, -4, 3, -2, 1, -8 };
    std::vector<int> sort_vec = { -8, -4, -2, 1, 3, 5, 6 };
    int number_threads = 2;
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_2) {
    int size = 20;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> std_sort_vec = vec;
    int number_threads = 2;
    std::sort(std_sort_vec.begin(), std_sort_vec.end());
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(std_sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_3) {
    int size = 20;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> std_sort_vec = vec;
    int number_threads = 4;
    std::sort(std_sort_vec.begin(), std_sort_vec.end());
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(std_sort_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_4) {
    int size = 100;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 2;
    radixSort(&seq_vec, seq_vec.size(), 0);
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(seq_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_5) {
    int size = 100;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 4;
    radixSort(&seq_vec, seq_vec.size(), 0);
    parallelRadixSort(&vec, vec.size(), number_threads);
    ASSERT_EQ(seq_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_6) {
    int size = 100000;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 4;
    double a = omp_get_wtime();
    radixSort(&seq_vec, seq_vec.size(), 0);
    double b = omp_get_wtime();
    std::cout << "seq time = " << b - a << std::endl;

    a = omp_get_wtime();
    parallelRadixSort(&vec, vec.size(), number_threads);
    b = omp_get_wtime();
    std::cout << "parallel time = " << b - a << std::endl;
    ASSERT_EQ(seq_vec, vec);
}

TEST(zotov_parralel_radix_sort, Parralel_Radix_Sort_7) {
    int size = 100000;
    std::vector<int> vec = getRandomVector(size);
    std::vector<int> seq_vec = vec;
    int number_threads = 8;
    double a = omp_get_wtime();
    radixSort(&seq_vec, seq_vec.size(), 0);
    double b = omp_get_wtime();
    std::cout << "seq time = " << b - a << std::endl;

    a = omp_get_wtime();
    parallelRadixSort(&vec, vec.size(), number_threads);
    b = omp_get_wtime();
    std::cout << "parallel time = " << b - a << std::endl;
    ASSERT_EQ(seq_vec, vec);
}


int main(int argc, char **argv) {
    ::testing::InitGoogleTest(&argc, argv);
    return RUN_ALL_TESTS();
}
\end{lstlisting}
\end{document}